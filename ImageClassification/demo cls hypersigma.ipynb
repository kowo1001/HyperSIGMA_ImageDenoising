{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ss_fusion_cls\n",
    "import torch\n",
    "from torch  import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,cohen_kappa_score\n",
    "from model import split_data,utils\n",
    "from sklearn import metrics, preprocessing\n",
    "from mmengine.optim import build_optim_wrapper\n",
    "from mmcv_custom import custom_layer_decay_optimizer_constructor,layer_decay_optimizer_constructor_vit\n",
    "import scipy.io as sio\n",
    "from thop import profile\n",
    "from multiprocessing import shared_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self):\n",
    "        self.data_cube = None\n",
    "        self.g_truth = None\n",
    "\n",
    "    @property\n",
    "    def cube(self):\n",
    "        \"\"\"\n",
    "        origin data\n",
    "        \"\"\"\n",
    "        return self.data_cube\n",
    "\n",
    "    @property\n",
    "    def truth(self):\n",
    "        return self.g_truth\n",
    "\n",
    "    @property\n",
    "    def normal_cube(self):\n",
    "        \"\"\"\n",
    "        normalization data: range(0, 1)\n",
    "        \"\"\"\n",
    "        return (self.data_cube - np.min(self.data_cube)) / (np.max(self.data_cube) - np.min(self.data_cube))\n",
    "class IndianRaw(DataReader):\n",
    "    def __init__(self):\n",
    "        super(IndianRaw, self).__init__()\n",
    "        raw_data_package = sio.loadmat(r\"data/Indian_pines_corrected.mat\")\n",
    "        self.data_cube = raw_data_package[\"data\"].astype(np.float32)\n",
    "        truth = sio.loadmat(r\"data/Indian_pines_gt.mat\")\n",
    "        self.g_truth = truth[\"groundT\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = IndianRaw().cube\n",
    "    data_gt = IndianRaw().truth\n",
    "    return data, data_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size =33\n",
    "patch_size = 2\n",
    "pca_components = 30\n",
    "split_type = ['number', 'ratio'][0]\n",
    "train_num =10\n",
    "val_num =5\n",
    "train_ratio = 0.98 \n",
    "val_ratio = 0.01 \n",
    "\n",
    "max_epoch = 100\n",
    "batch_size = 16\n",
    "learning_rate = 0.00001 \n",
    "dataset_name = 'HH'\n",
    "\n",
    "path_weight = r\"weights//\"\n",
    "path_result = r\"result//\"\n",
    "data, data_gt = load_data()\n",
    "height, width, bands = data.shape\n",
    "gt_reshape = np.reshape(data_gt, [-1])\n",
    "class_num = np.max(data_gt)\n",
    "class_num = class_num.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_gt = load_data()\n",
    "train_index, val_index, test_index = split_data.split_data(gt_reshape, \n",
    "            class_num, train_ratio, train_ratio, train_num, val_num, split_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pca = split_data.apply_PCA(data, num_components=pca_components)\n",
    "data_all, data_label_all = split_data.create_patches(data, data_gt, window_size=img_size, remove_zero_labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=train_index.astype(int)\n",
    "val_index=val_index.astype(int)\n",
    "test_index=test_index.astype(int)\n",
    "height, width, bands = data.shape\n",
    "gt_reshape = np.reshape(data_gt, [-1])\n",
    "class_num = np.max(gt_reshape)\n",
    "class_num = class_num.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index0, val_index0, re_index = split_data.split_data(gt_reshape, \n",
    "            class_num, 0, 0, 0, 10,'ratio' )\n",
    "re_index = np.sort(re_index)\n",
    "\n",
    "train_index = train_index.reshape(-1,1)\n",
    "val_index = val_index.reshape(-1,1)\n",
    "test_index = test_index.reshape(-1,1)\n",
    "\n",
    "train_label = data_label_all[train_index].reshape(-1,)\n",
    "val_label = data_label_all[val_index].reshape(-1,)\n",
    "test_label = data_label_all[test_index]\n",
    "\n",
    "train = data_all[train_index,:,:,:]\n",
    "val = data_all[val_index,:,:,:]\n",
    "test = data_all[test_index,:,:,:]\n",
    "\n",
    "split_data.data_info(train_label, val_label, test_label, np.max(data_label_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reshape(-1, img_size, img_size, bands)\n",
    "val = val.reshape(-1, img_size, img_size, bands)\n",
    "test = test.reshape(-1,img_size, img_size, bands)\n",
    "print('before transpose: train shape: ', train.shape)\n",
    "print('before transpose: test  shape: ', val.shape)\n",
    "print('before transpose: test  shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.transpose(0, 3, 1, 2)\n",
    "val = val.transpose(0,  3, 1,2 )\n",
    "test = test.transpose(0,  3, 1,2 )\n",
    "print('after transpose: train shape: ', train.shape)\n",
    "print('after transpose: val  shape: ', val.shape)\n",
    "print('after transpose: test  shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hi_gt= torch.zeros(train_label.size,class_num)\n",
    "val_hi_gt= torch.zeros(val_label.size,class_num)\n",
    "test_hi_gt= torch.zeros(test_label.size,class_num)\n",
    "\n",
    "\n",
    "train_hi_gt = torch.LongTensor(train_hi_gt.numpy())\n",
    "test_hi_gt = torch.LongTensor(test_hi_gt.numpy())\n",
    "val_hi_gt = torch.LongTensor(val_hi_gt.numpy())\n",
    "\n",
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = train.shape[0]\n",
    "        self.x_data = torch.FloatTensor(train)\n",
    "        self.y_data = torch.LongTensor(train_label)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\"\"\" Val dataset\"\"\"\n",
    "\n",
    "\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = val.shape[0]\n",
    "        self.x_data = torch.FloatTensor(val)\n",
    "        self.y_data = torch.LongTensor(val_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "\n",
    "\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = test.shape[0]\n",
    "        self.x_data = torch.FloatTensor(test)\n",
    "        self.y_data = torch.LongTensor(test_label)  \n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 创建 trainloader 和 testloader\n",
    "trainset = TrainDS()\n",
    "valset = ValDS()\n",
    "testset = TestDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=valset, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ss_fusion_cls.SSFusionFramework(\n",
    "                img_size = img_size,\n",
    "                in_channels = pca_components,\n",
    "                patch_size=patch_size,\n",
    "                classes = class_num+1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params =model.state_dict()\n",
    "spat_net = torch.load((r\"spat-base.pth\"), map_location=torch.device('cpu'))\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_map' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_output_maps' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'pos_embed' in k:\n",
    "        del spat_net['model'][k]\n",
    "spat_weights = {}\n",
    "prefix = 'spat_encoder.'\n",
    "for key, value in spat_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spat_weights[new_key] = value\n",
    "per_net = torch.load((r\"spec-base.pth\"), map_location=torch.device('cpu'))\n",
    "model_params =model.state_dict()\n",
    "for k in list(per_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'spat_map' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'fpn1.0.weight' in k:\n",
    "        del per_net['model'][k]\n",
    "spec_weights = {}\n",
    "prefix = 'spec_encoder.'\n",
    "for key, value in per_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spec_weights[new_key] = value\n",
    "model_params =model.state_dict()\n",
    "for k in list(spec_weights.keys()):\n",
    "    if 'spec_encoder.patch_embed' in k:\n",
    "        del spec_weights[k]\n",
    "merged_params = {**spat_weights, **spec_weights}\n",
    "same_parsms = {k: v for k, v in merged_params.items() if k in model_params.keys()}\n",
    "model_params.update(same_parsms)\n",
    "model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_wrapper = dict(\n",
    "    optimizer=dict(\n",
    "    type='AdamW', lr=6e-5, betas=(0.9, 0.999), weight_decay=0.05),\n",
    "    constructor='LayerDecayOptimizerConstructor_ViT', \n",
    "    paramwise_cfg=dict(\n",
    "        num_layers=12, \n",
    "        layer_decay_rate=0.9,\n",
    "        )\n",
    "        )\n",
    "optimizer = build_optim_wrapper(model, optim_wrapper)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.optimizer, 300, eta_min=0, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "count = 0\n",
    "best_loss = 99999\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch+ 1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    _train_loss = 0\n",
    "    val_correct = 0\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        train_loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        _train_loss += train_loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct = (y_pred == y).sum().item()\n",
    "            total = y.size(0)\n",
    "\n",
    "    train_losses.append(train_loss.cpu().detach().item())\n",
    "    \n",
    "    if epoch % 10==0:\n",
    "       \n",
    "        epoch_loss = _train_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), path_weight + \"model.pth\")\n",
    "            torch.save(optimizer.state_dict(), path_weight + 'optimizer.pth')\n",
    "            print('save model')\n",
    "\n",
    "        \n",
    "        print('epoch', epoch,\n",
    "            'loss: ', round(epoch_loss, 5),\n",
    "            'accuracy: ', round(epoch_accuracy, 5),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      7\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m test_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (y_pred \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hypersigma/model/ss_fusion_cls.py:131\u001b[0m, in \u001b[0;36mSSFusionFramework.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    129\u001b[0m     b, _, h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 131\u001b[0m     img_feature1,img_feature2,img_feature3,img_feature4\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspat_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# spec\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     spec_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec_encoder(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hypersigma/model/SpatViT_fusion_patch.py:685\u001b[0m, in \u001b[0;36mSpatViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    684\u001b[0m     (a,c, h, w) \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 685\u001b[0m     feture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     feture1 \u001b[38;5;241m=\u001b[39m feture[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m    687\u001b[0m     feture2 \u001b[38;5;241m=\u001b[39m feture[\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/hypersigma/model/SpatViT_fusion_patch.py:669\u001b[0m, in \u001b[0;36mSpatViT.forward_features\u001b[0;34m(self, x, patch_size)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint:\n\u001b[0;32m--> 669\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeform_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m         x \u001b[38;5;241m=\u001b[39m blk(x, Hp, Wp, deform_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/checkpoint.py:235\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m kwargs))\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    238\u001b[0m         function,\n\u001b[1;32m    239\u001b[0m         preserve,\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;241m*\u001b[39margs\n\u001b[1;32m    241\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/checkpoint.py:96\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     93\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 96\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hypersigma/model/SpatViT_fusion_patch.py:295\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, H, W, deform_inputs)\u001b[0m\n\u001b[1;32m    293\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeform_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    296\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/hypersigma/model/SpatViT_fusion_patch.py:164\u001b[0m, in \u001b[0;36mSampleAttention.forward\u001b[0;34m(self, x, H, W, deform_inputs)\u001b[0m\n\u001b[1;32m    161\u001b[0m q \u001b[38;5;241m=\u001b[39m q[:,:,:,\u001b[38;5;28;01mNone\u001b[39;00m,:] \u001b[38;5;66;03m# B, H, N, 1, C\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# B*H, c, N, P\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m sampled_k \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzeros\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, L \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, N, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_points)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# 根据位置从每个level的特征图中采样出点作为value\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# B*H, c, N, P\u001b[39;00m\n\u001b[1;32m    168\u001b[0m sampled_v \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgrid_sample(v, sampling_locations, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    169\u001b[0m                                   padding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, L \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, N, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_points)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# 根据位置从每个level的特征图中采样出点作为value\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:4223\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   4216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign_corners=True if the old behavior is desired. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee the documentation of grid_sample for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4220\u001b[0m     )\n\u001b[1;32m   4221\u001b[0m     align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "model.load_state_dict(torch.load(path_weight + r\"model.pth\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y  in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        test_correct += (y_pred == y).sum().item()\n",
    "        test_total += y.size(0)\n",
    "\n",
    "        if count == 0:\n",
    "            y_pred_test =  y_pred.cpu().numpy()\n",
    "            y_gt_test = y.cpu().numpy()\n",
    "            count = 1\n",
    "        else:\n",
    "            y_pred_test = np.concatenate( (y_pred_test, y_pred.cpu().numpy()) )\n",
    "            y_gt_test = np.concatenate( (y_gt_test, y.cpu().numpy()) )\n",
    "\n",
    "overall_acc = metrics.accuracy_score(y_pred_test, y_gt_test)\n",
    "confusion_matrix = metrics.confusion_matrix(y_pred_test, y_gt_test)\n",
    "each_acc, average_acc = utils.aa_and_each_accuracy(confusion_matrix)\n",
    "kappa = metrics.cohen_kappa_score(y_pred_test, y_gt_test)\n",
    "OA_HybirdSN = test_correct / test_total\n",
    "print(\"test OA={:.4f}\".format(overall_acc))\n",
    "print('kappa=',kappa)\n",
    "print('each_acc=',each_acc)\n",
    "print('average_acc=',average_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
